{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import PIL\n",
    "import time\n",
    "import copy\n",
    "import scipy\n",
    "import sklearn\n",
    "import math\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from utils.data_io import *\n",
    "\n",
    "from opts import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        if m.weight is not None:\n",
    "            nn.init.kaiming_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            init.constant(m.bias, 0.0)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        if m.weight is not None:\n",
    "            init.constant(m.weight, 1.0)\n",
    "        if m.bias is not None:\n",
    "            init.constant(m.bias, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(swish, self).__init__()\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return z*nn.Relu(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(features))\n",
    "        self.beta = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # mean = x.mean(-1, keepdim=True)\n",
    "        # std = x.std(-1, keepdim=True)\n",
    "        # pdb.set_trace()\n",
    "        # return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "        shape = [-1] + [1] * (x.dim() - 1)\n",
    "        mean = x.view(x.size(0), -1).mean(1).view(*shape)\n",
    "        std = x.view(x.size(0), -1).std(1).view(*shape)\n",
    "        # print (\"to x {}\".format(x.data.numpy().shape))\n",
    "        # print (\"to gamma {}\".format(self.gamma.shape))\n",
    "        # print (\"to beta {}\".format(self.beta.shape))\n",
    "        # print (\"to mean {}\".format(mean.data.numpy().shape))\n",
    "        # print (\"to std {}\".format(std.data.numpy().shape))\n",
    "\n",
    "        y = (x - mean) / (std + self.eps)\n",
    "        shape = [1, -1] + [1] * (x.dim() - 2)\n",
    "        y = self.gamma.view(*shape) * y + self.beta.view(*shape)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"same padding\" padding = int((kernel_size-1)/2)\n",
    "class ConvPadding(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True):\n",
    "        super(ConvPadding, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = int((kernel_size - 1)/2)\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        self.bias = bias\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = nn.Conv2d(self.in_channels, self.out_channels, self.kernel_size, self.stride, self.padding, self.dilation, \n",
    "                        self.groups, self.bias)(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvMeanPool(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
    "        super(ConvMeanPool, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        self.bias = bias\n",
    "        self.conv = ConvPadding(self.in_channels, self.out_channels, self.kernel_size, self.stride, self.dilation, \n",
    "                        self.groups, self.bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        conv = ConvPadding(x)\n",
    "        \n",
    "        #::k ervery k element, s=range(20), s[::3]=[0, 3, 6, 9, 12, 15, 18]\n",
    "        #L[x::y] means a slice of L where the x is the index to start from and y is the step size.\n",
    "        #here we \n",
    "        out = ([conv[:,:,::2,::2] + conv[:,:,1::2,::2] + conv[:,:,::2,1::2] + conv[:,:,1::2,1::2]])/4.\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPoolConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
    "        super(MeanPoolConv, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        self.bias = bias\n",
    "        self.conv = ConvPadding(self.in_channels, self.out_channels, self.kernel_size, self.stride, self.dilation, \n",
    "                        self.groups, self.bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = ([x[:,:,::2,::2] + x[:,:,1::2,::2] + x[:,:,::2,1::2] + x[:,:,1::2,1::2]])/4.\n",
    "        out = ConvPadding(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rearranges data from depth into blocks of spatial data. This is the reverse transformation of SpaceToDepth\n",
    "#This operation is useful for resizing the activations between convolutions (but keeping all data), \n",
    "#e.g. instead of pooling. It is also useful for training purely convolutional models.\n",
    "#N H W C\n",
    "#Chunks of data of size block_size * block_size from depth \n",
    "#are rearranged into non-overlapping blocks of size block_size x block_size\n",
    "#The width the output tensor is input_depth * block_size, whereas the height is input_height * block_siz\n",
    "class DepthToSpace(nn.Module):\n",
    "    def __init__(self, block_size):\n",
    "        super(DepthToSpace, self).__init__()\n",
    "        self.block_size = block_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 3, 1) #N H W C\n",
    "        (batch_size, in_height, in_width, in_channels) = x.size()\n",
    "        out_channels = int(in_channels / self.block_size / self.block_size)\n",
    "        out_width = int(in_width * self.block_size)\n",
    "        out_height = int(in_height * self.block_size)\n",
    "        out = x.reshape(batch_size, input_height, input_width, self.block_size*self.block_size, out_channels)\n",
    "        #N H W BLOCK*BLOCK C/BLOCK/BLOCK\n",
    "        \n",
    "        splits = out.split(self.block_size, dim=3)\n",
    "        #BLOCK (N H W BLOCK C/BLOCK/BLOCK) list\n",
    "        \n",
    "        #If split_size_or_sections is an integer type, then tensor will be split into equally sized chunks (if possible). \n",
    "        #Last chunk will be smaller if the tensor size along the given dimension dim is not divisible by split_size.\n",
    "\n",
    "        #If split_size_or_sections is a list, then tensor will be split into len(split_size_or_sections) chunks \n",
    "        #with sizes in dim according to split_size_or_sections.\n",
    "        \n",
    "        #split -> (N H W BLOCK C/BLOCK/BLOCK) -> reshape (N H W*BLOCK C/BLOCK/BLOCK)\n",
    "        #stacks -> BLOCK (N H W*BLOCK C/BLOCK/BLOCK)\n",
    "        stacks = [split.reshape(batch_size, in_height, out_width, out_channels) for split in splits]\n",
    "        #stacks -> BLOCK N H W*BLOCK C/BLOCK/BLOCK\n",
    "        stacks = torch.stack(stacks, 0)\n",
    "        #stacks -> N BLOCK H W*BLOCK C/BLOCK/BLOCK\n",
    "        stacks = stacks.transpose(0, 1)\n",
    "        #stacks -> N H*BLOCK W*BLOCK C/BLOCK/BLOCK\n",
    "        stacks = stacks.reshape(batch_size, out_height, out_width, out_channels)\n",
    "        #out -> N C/BLOCK/BLOCK H*BLOCK W*BLOCK\n",
    "        out = stacks.permute(0, 3, 1, 2)\n",
    "        \n",
    "        return out       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpsamplingConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
    "        super(UpsamplingConv, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        self.bias = bias\n",
    "        self.depth_to_space = DepthToSpace(2)\n",
    "        self.conv = ConvPadding(self.in_channels, self.out_channels, self.kernel_size, self.stride, self.dilation, \n",
    "                        self.groups, self.bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = torch.cat((x, x, x, x), 1)\n",
    "        #x -> N H W C*4\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        #x -> N C H*2 W*2\n",
    "        x = self.depth_to_space(x)\n",
    "        out = ConvPadding(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, resample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        self.bias = bias\n",
    "        self.resample = resample\n",
    "        self.bn1 = None\n",
    "        self.bn2 = None\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        if resample == 'down':\n",
    "            self.bn1 = nn.LayerNorm(in_channels)\n",
    "            self.bn2 = nn.LayerNorm(in_channels)\n",
    "        elif resample == 'up':\n",
    "            self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "            self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        elif resample == None:\n",
    "            self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "            self.bn2 = nn.LayerNorm(out_channels)\n",
    "        else:\n",
    "            raise Exception('invalid resample value')\n",
    "\n",
    "        if resample == 'down':\n",
    "            \n",
    "            self.conv_shortcut = MeanPoolConv(self.in_channels, self.out_channels, 1, self.stride, \n",
    "                                             self.padding, self.dilation, self.groups, self.bias)\n",
    "            \n",
    "            self.conv_1 = ConvPadding(self.in_channels, self.in_channels, self.kernel_size, self.stride, self.dilation, \n",
    "                        self.groups, bias=False)\n",
    "            \n",
    "            self.conv_2 = ConvMeanPool(self.in_channels, self.out_channels, self.kernel_size, self.stride, \n",
    "                                             self.padding, self.dilation, self.groups, self.bias)\n",
    "        \n",
    "        elif resample == 'up':\n",
    "            \n",
    "            self.conv_shortcut = UpSampleConv(self.in_channels, self.out_channels, 1, self.stride, \n",
    "                                             self.padding, self.dilation, self.groups, self.bias)\n",
    "            \n",
    "            self.conv_1 = UpSampleConv(self.in_channels, self.out_channels, self.kernel_size, self.stride, \n",
    "                                             self.padding, self.dilation, self.groups, bias=False)\n",
    "            \n",
    "            self.conv_2 = MyConvo2d(self.out_channels, self.out_channels, self.kernel_size, self.stride, self.dilation, \n",
    "                        self.groups, self.bias)\n",
    "        \n",
    "        elif resample == None:\n",
    "            \n",
    "            self.conv_shortcut = MyConvo2d(self.in_channels, self.out_channels, 1, self.stride, self.dilation, \n",
    "                        self.groups, self.bias)\n",
    "            \n",
    "            self.conv_1 = MyConvo2d(self.in_channels, self.in_channels, self.kernel_size, self.stride, self.dilation, \n",
    "                        self.groups, bias=False)\n",
    "            \n",
    "            self.conv_2 = MyConvo2d(self.in_channels, self.out_channels, self.kernel_size, self.stride, self.dilation, \n",
    "                        self.groups, self.bias)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            raise Exception('invalid resample value')\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.input_dim == self.output_dim and self.resample == None:\n",
    "            shortcut = input\n",
    "        else:\n",
    "            shortcut = self.conv_shortcut(input)\n",
    "\n",
    "        output = input\n",
    "        output = self.bn1(output)\n",
    "        output = self.relu1(output)\n",
    "        output = self.conv_1(output)\n",
    "        output = self.bn2(output)\n",
    "        output = self.relu2(output)\n",
    "        output = self.conv_2(output)\n",
    "\n",
    "        return shortcut + output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Good_Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3, dim=64):\n",
    "        super(Good_Discriminator, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.dim = dim\n",
    "        \n",
    "        self.conv1 = ConvPadding(self.in_channels, dim, 3, 1)\n",
    "        self.resblock1 = ResidualBlock(self.dim, 2*self.dim, 3, 'down')\n",
    "        self.resblock2 = ResidualBlock(2*self.dim, 4*self.dim, 3, 'down')\n",
    "        self.resblock3 = ResidualBlock(4*self.dim, 8*self.dim, 3, 'down')\n",
    "        self.resblock4 = ResidualBlock(8*self.dim, 8*self.dim, 3, 'down')\n",
    "        \n",
    "        self.linear = nn.Linear(4*4*8*self.dim, 1)\n",
    "        \n",
    "    def forward(self, x, img_size=64):\n",
    "        \n",
    "        x = x.contiguous()\n",
    "        #N H W C -> N C H W\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "       \n",
    "        x = x.view(-1, self.in_channels, img_size, img_size)\n",
    "        \n",
    "        conv1 = self.conv1(x)\n",
    "        #no activation, why???\n",
    "        \n",
    "        res1 = self.resblock1(conv1)\n",
    "        res2 = self.resblock2(res1)\n",
    "        res3 = self.resblock3(res2)\n",
    "        res4 = self.resblock4(res4)\n",
    "        \n",
    "        out = res.view(-1, 4*4*8*self.dim)\n",
    "        out = self.linear(out)\n",
    "        out = out.view(-1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class INN_Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3, dim=64, batch_norm=True):\n",
    "        super(INN_Discriminator, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.dim = dim\n",
    "        self.batch_norm = batch_norm\n",
    "        \n",
    "        self.conv1 = ConvPadding(self.in_channels, dim/2, 3, 1)\n",
    "        self.swish = swish()\n",
    "        \n",
    "        self.conv2 = ConvPadding(dim/2, dim, 3, 1)\n",
    "        self.layernorm1 = nn.LayerNorm(dim)\n",
    "        \n",
    "        self.meanpoolconv1 = MeanPoolConv(dim, dim, 3, 1)\n",
    "        self.layernorm2 = nn.LayerNorm(dim)\n",
    "        \n",
    "        self.conv3 = ConvPadding(dim, dim*2, 3, 1)\n",
    "        self.layernorm3 = LayerNorm(dim*2)\n",
    "        \n",
    "        self.meanpoolconv2 = MeanPoolConv(dim*2, dim*2, 3, 1)\n",
    "        self.layernorm4 = nn.LayerNorm(dim*2)\n",
    "        \n",
    "        self.conv4 = ConvPadding(dim*2, dim*4, 3, 1)\n",
    "        self.layernorm5 = nn.LayerNorm(dim*4)\n",
    "        \n",
    "        self.meanpoolconv3 = MeanPoolConv(dim*4, dim*4, 3, 1)\n",
    "        self.layernorm6 = nn.LayerNorm(dim*4)\n",
    "        \n",
    "        self.conv5 = ConvPadding(dim*4, dim*8, 3, 1)\n",
    "        self.layernorm7 = nn.LayerNorm(dim*8)\n",
    "        self.swish8 = swish()\n",
    "        \n",
    "        self.linear = nn.Linear(4*4*8*dim, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, img_size=64):\n",
    "        \n",
    "        x = x.contiguous()\n",
    "        #N H W C -> N C H W\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "       \n",
    "        x = x.view(-1, self.in_channels, img_size, img_size)\n",
    "        \n",
    "        conv1 = self.swish(self.conv1(x))\n",
    "        #N self.dim/2 img_size img_size\n",
    "        \n",
    "        if(self.batch_norm):\n",
    "            conv2 = self.swish(self.layernorm1(self.conv2(x)))\n",
    "            #N self.dim img_size img_size\n",
    "        else:\n",
    "            conv2 = self.swish(self.conv2(x))\n",
    "            #N self.dim img_size img_size\n",
    "            \n",
    "        if(self.batch_norm):\n",
    "            meanpoolconv1 = self.swish(self.layernorm2(self.meanpoolconv1(conv2)))\n",
    "            #N self.dim img_size/2 img_size/2\n",
    "        else:\n",
    "            meanpoolconv1 = self.swish(self.meanpoolconv1(conv2))\n",
    "            #N self.dim img_size/2 img_size/2\n",
    "        \n",
    "        if(self.batch_norm):\n",
    "            conv3 = self.swish(self.layernorm3(self.conv3(meanpoolconv1)))\n",
    "            #N self.dim*2 img_size/2 img_size/2\n",
    "        else:\n",
    "            conv3 = self.swish(self.conv3(meanpoolconv1))\n",
    "            #N self.dim*2 img_size/2 img_size/2\n",
    "        \n",
    "        if(self.batch_norm):\n",
    "            meanpoolconv2 = self.swish(self.layernorm4(self.meanpoolconv2(conv3)))\n",
    "            #N self.dim*2 img_size/4 img_size/4\n",
    "        else:\n",
    "            meanpoolconv2 = self.swish(self.meanpoolconv2(conv3))\n",
    "            #N self.dim*2 img_size/4 img_size/4\n",
    "        \n",
    "        if(self.batch_norm):\n",
    "            conv4 = self.swish(self.layernorm5(self.conv4(meanpoolconv2)))\n",
    "            #N self.dim*4 img_size/4 img_size/4\n",
    "        else:\n",
    "            conv4 = self.swish(self.conv4(meanpoolconv2))\n",
    "            #N self.dim*4 img_size/4 img_size/4\n",
    "        \n",
    "        if(self.batch_norm):\n",
    "            meanpoolconv3 = self.swish(self.layernorm6(self.meanpoolconv3(conv4)))\n",
    "            #N self.dim*4 img_size/8 img_size/8\n",
    "        else:\n",
    "            meanpoolconv3 = self.swish(self.meanpoolconv3(conv4))\n",
    "            #N self.dim*4 img_size/8 img_size/8\n",
    "        \n",
    "        if(self.batch_norm):\n",
    "            conv5 = self.swish(self.layernorm7(self.conv5(meanpoolconv3)))\n",
    "            #N self.dim*8 img_size/8 img_size/8 \n",
    "        else:\n",
    "            conv5 = self.swish(self.conv5(meanpoolconv3))\n",
    "            #N self.dim*8 img_size/8 img_size/8 \n",
    "        \n",
    "        out = (conv5[:, :, ::2, ::2] + conv5[:, :, 1::2, ::2] + conv5[:, :, ::2, 1::2] + conv5[:, :, 1::2, 1::2])/4\n",
    "        #N self.dim*8 img_size/16 img_size/16 \n",
    "        \n",
    "        out = out.view(-1, (img_size/16)*(img_size/16)*8*self.dim)\n",
    "        \n",
    "        out = self.linear(out)\n",
    "        #N 1\n",
    "        \n",
    "        out = out.view(-1)\n",
    "        #N\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Noise samples, initial psudo negatives\n",
    "class Noise(nn.Module):\n",
    "    def __init__(self, n_samples, dim = 64):\n",
    "        super(Noise, self).__init__()\n",
    "        \n",
    "        self.n_samples = n_samples\n",
    "        self.dim = dim\n",
    "        self.conv1 = ConvPaddings(8*self.dim, 4*self.dim, 5, 1)\n",
    "        self.upsample1 = nn.Upsample(size=8, scale_factor=None, mode='nearest', align_corners=None)\n",
    "        self.layernorm1 = nn.LayerNorm(4*self.dim)\n",
    "        \n",
    "        self.conv2 = ConvPaddings(4*self.dim, 2*self.dim, 5, 1)\n",
    "        self.upsample2 = nn.Upsample(size=16, scale_factor=None, mode='nearest', align_corners=None)\n",
    "        self.layernorm2 = nn.LayerNorm(2*self.dim)\n",
    "        \n",
    "        self.conv3 = ConvPaddings(2*self.dim, self.dim, 5, 1)\n",
    "        self.upsample3 = nn.Upsample(size=32, scale_factor=None, mode='nearest', align_corners=None)\n",
    "        self.layernorm3 = nn.LayerNorm(self.dim)\n",
    "        \n",
    "        self.conv4 = ConvPaddings(self.dim, 3, 5, 1)\n",
    "        self.upsample4 = nn.Upsample(size=64, scale_factor=None, mode='nearest', align_corners=None)\n",
    "        self.layernorm4 = nn.LayerNorm(3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        conv1 = self.conv1(x)\n",
    "        upsample1 = self.upsample1(conv1)\n",
    "        upsample1 = self.layernorm1(upsample1)\n",
    "        #N 4*dim 8 8\n",
    "        \n",
    "        conv2 = self.conv2(x)\n",
    "        upsample2 = self.upsample2(conv2)\n",
    "        upsample2 = self.layernorm2(upsample2)\n",
    "        #N 2*dim 16 16\n",
    "        \n",
    "        conv3 = self.conv3(x)\n",
    "        upsample3 = self.upsample3(conv3)\n",
    "        upsample3 = self.layernorm3(upsample3)\n",
    "        #N dim 32 32\n",
    "        \n",
    "        conv4 = self.conv4(x)\n",
    "        upsample4 = self.upsample4(conv4)\n",
    "        upsample4 = self.layernorm4(upsample4)\n",
    "        #N dim/2 64 64\n",
    "        \n",
    "        return upsample4\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WINN(nn.Module):\n",
    "    def __init__(self, in_channels=3, dim=64):\n",
    "        super(WINN, self).__init__()\n",
    "        self.batch_size = opts.batch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.img_size = img_size\n",
    "        self.dim = dim\n",
    "        self.img_size = opts.img_size\n",
    "        self.num_chain = opts.nRow*opts.nCol #each image in final result\n",
    "        self.opts = opts\n",
    "        \n",
    "        if(opts.with_noise):\n",
    "            print(\"Langevin Dynamics with noise\")\n",
    "        else:\n",
    "            print(\"Langevin Dynamics without noise\")\n",
    "        \n",
    "        if(opts.set=='cifar'):\n",
    "            opts.img_size = 32\n",
    "            print(\"training on cifar with image size: %i\" %(img_size))\n",
    "            \n",
    "        \n",
    "    def load_Discriminator(self, file):\n",
    "        self.discriminator = torch.load(file).train()\n",
    "        print('Loading Descriptor from ' + file + '...')\n",
    "        \n",
    "        \n",
    "    def langevin_dynamics_discriminator(self, x):\n",
    "        \n",
    "        #run langevin_step_num_gen steps langevin dynamics\n",
    "        for i in range(self.opts.langevin_step_num_des):\n",
    "            \n",
    "            #dimension of x is 3\n",
    "            noise = Variable(torch.randn(self.num_chain, 3, self.opts.img_size, self.opts.img_size).cuda())\n",
    "            #\"However, .data can be unsafe in some cases. \n",
    "            #Any changes on x.data wouldn’t be tracked by autograd, \n",
    "            #and the computed gradients would be incorrect if x is needed in a backward pass. \n",
    "            #A safer alternative is to use x.detach(), \n",
    "            #which also returns a Tensor that shares data with requires_grad=False, \n",
    "            #but will have its in-place changes reported by autograd if x is needed in backward.\"\n",
    "            \n",
    "            # clone it and turn x into a leaf variable so the grad won't be thrown away\n",
    "            x = Variable(x.data, requires_grad=True)\n",
    "            \n",
    "            #gradient is torch.ones(self.num_chain, self.opts.z_size).cuda()\n",
    "            \n",
    "            x_feature = self.discriminator(x)\n",
    "            #x_feature is f(x;\\theta) which is \\ln(p(y=1|x,\\theta)/p(y=0|x,\\theta))\n",
    "            \n",
    "            #torch.autograd.backward(tensors, grad_tensors=None, retain_graph=None, \n",
    "            #create_graph=False, grad_variables=None)\n",
    "            \n",
    "            # do backward for all element of x_feature\n",
    "            x_feature.backward(torch.ones(self.num_chain, self.opts.z_size).cuda())\n",
    "            \n",
    "            #grad = \\frac{\\partial f(x;\\theta)}{\\partial x}\n",
    "            grad = x.grad\n",
    "            \n",
    "            # print ('x is : '+str(x[0]))\n",
    "            # print ('x_grad is : '+str(grad[0]))\n",
    "            \n",
    "            x = x + 0.5 * self.opts.langevin_step_size_dis * self.opts.langevin_step_size_dis * grad\n",
    "            \n",
    "            #+ step_size*U_{\\tau}\n",
    "            if self.opts.with_noise:\n",
    "                x += self.opts.langevin_step_size_dis * noise\n",
    "                \n",
    "        return x \n",
    "        \n",
    "    def train(self, discriminator_model=None, LAMBDA=10.0):\n",
    "        if(discriminator_model!=None):\n",
    "            self.discriminator = torch.load(file).train()\n",
    "            print('Loading Discriminator from ' + discriminator_model + '...')\n",
    "        else:\n",
    "            self.discriminator = INN_Discriminator().cuda().train()\n",
    "            print('Loading Discriminator without initialization...')\n",
    "            \n",
    "            \n",
    "        if self.opts.set == 'scene' or self.opts.set == 'cifar':\n",
    "            train_data = DataSet(os.path.join(self.opts.data_path, self.opts.category), \n",
    "                                 image_size=self.opts.img_size)\n",
    "        else:\n",
    "            train_data = torchvision.datasets.LSUN(root=self.opts.data_path,\n",
    "                                                   classes=['bedroom_train'],\n",
    "                                                   transform=transforms.Compose([transforms.Resize(self.img_size),\n",
    "                                                   transforms.ToTensor(), ]))\n",
    "            \n",
    "        num_batches = int(math.ceil(len(train_data) / batch_size))\n",
    "        \n",
    "        if not os.path.exists(self.opts.ckpt_dir):\n",
    "            os.makedirs(self.opts.ckpt_dir)\n",
    "        if not os.path.exists(self.opts.output_dir):\n",
    "            os.makedirs(self.opts.output_dir)\n",
    "        logfile = open(self.opts.ckpt_dir + '/log', 'w+')\n",
    "        \n",
    "        # Prepare for root directory of intermediate image.\n",
    "        intermediate_image_root = os.path.join(self.opts.output_dir, \"intermediate\")\n",
    "        mkdir_if_not_exists(intermediate_image_root)\n",
    "        # Prepare for root directory of negative images.\n",
    "        neg_image_root = os.path.join(self.opts.output_dir, \"negative\")\n",
    "        mkdir_if_not_exists(neg_image_root)\n",
    "        \n",
    "        ######################################################################\n",
    "        # Training stage 1: Load positive images.\n",
    "        ######################################################################\n",
    "        log(log_file_path,\n",
    "            \"Training stage 1: Load positive images...\")\n",
    "\n",
    "\n",
    "        # sample_results = np.random.randn(self.num_chain * num_batches, self.opts.img_size, self.opts.img_size, 3)\n",
    "        dis_optimizer = torch.optim.Adam(self.discriminator.parameters(), lr=self.opts.lr_des,\n",
    "                                         betas=[self.opts.beta1_des, 0.999])\n",
    "        \n",
    "        D_pos_logits = self.discriminator(D_pos_images)\n",
    "        D_neg_logits = self.discriminator(D_neg_images)\n",
    "        \n",
    "        D_loss = torch.mean(D_neg_logits - D_pos_logits)\n",
    "        D_pos_loss = torch.mean(D_pos_logits)\n",
    "\n",
    "        uniform_dist = torch.distributions.uniform.Uniform(torch.Tensor([0.0]),torch.Tensor([1.0]))\n",
    "                            \n",
    "        epsilon = uniform_dist.sample(torch.Size([half_b_size, 1, 1, 1]))\n",
    "        # Dirty hack to tile the tensor\n",
    "        epsilon = epsilon + torch.zeros(D_pos_images.shape, dtype=epsilon.dtype)\n",
    "        x_hat = epsilon * D_pos_images + (1 - epsilon) * D_neg_images\n",
    "                                      \n",
    "        d_hat = self.descriptor(x_hat)\n",
    "\n",
    "        ddx = tf.gradients(d_hat, x_hat)[0]\n",
    "        ddx = torch.sqrt(torch.sum(ddx**2, (1, 2, 3)))\n",
    "        ddx = torch.sum((ddx - 1.0).pow(2) * LAMBDA)\n",
    "        D_loss += ddx\n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-num_epoch NUM_EPOCH]\n",
      "                             [-batch_size BATCH_SIZE] [-nRow NROW]\n",
      "                             [-nCol NCOL] [-img_size IMG_SIZE]\n",
      "                             [-test_size TEST_SIZE] [-test] [-score]\n",
      "                             [-z_size Z_SIZE] [-category CATEGORY]\n",
      "                             [-data_path DATA_PATH] [-output_dir OUTPUT_DIR]\n",
      "                             [-log_dir LOG_DIR] [-ckpt_dir CKPT_DIR]\n",
      "                             [-log_epoch LOG_EPOCH] [-set SET]\n",
      "                             [-with_noise WITH_NOISE]\n",
      "                             [-incep_interval INCEP_INTERVAL]\n",
      "                             [-ckpt_des CKPT_DES] [-sigma_gen SIGMA_GEN]\n",
      "                             [-langevin_step_num_gen LANGEVIN_STEP_NUM_GEN]\n",
      "                             [-langevin_step_size_gen LANGEVIN_STEP_SIZE_GEN]\n",
      "                             [-lr_gen LR_GEN] [-beta1_gen BETA1_GEN]\n",
      "                             [-ckpt_gen CKPT_GEN] [-sigma_des SIGMA_DES]\n",
      "                             [-langevin_step_num_des LANGEVIN_STEP_NUM_DES]\n",
      "                             [-langevin_step_size_des LANGEVIN_STEP_SIZE_DES]\n",
      "                             [-lr_des LR_DES] [-beta1_des BETA1_DES]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /run/user/1000/jupyter/kernel-f223f87c-fccd-4e5e-94c1-55cd569a6be2.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jionie/py3env/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3275: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
