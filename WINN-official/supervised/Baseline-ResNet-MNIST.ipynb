{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T21:50:19.626462Z",
     "start_time": "2018-04-08T21:50:18.524331Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = 'Fan Fan, Kwonjoon Lee and Weijian Xu'\n",
    "\n",
    "# Python libraries.\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "from datetime import datetime\n",
    "\n",
    "# Libraries related to tflib.\n",
    "import functools\n",
    "import tflib as lib\n",
    "import tflib.ops.linear\n",
    "import tflib.ops.conv2d\n",
    "import tflib.ops.batchnorm\n",
    "import tflib.ops.deconv2d\n",
    "import tflib.save_images\n",
    "import tflib.small_imagenet\n",
    "import tflib.ops.layernorm\n",
    "import tflib.plot\n",
    "\n",
    "# Custom libraries.\n",
    "from Utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T21:50:19.637289Z",
     "start_time": "2018-04-08T21:50:19.628377Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defined hyper-parameters.\n",
    "\n",
    "# Number of units in ResNet structure. 5 for ResNet-32.\n",
    "units      = 5 \n",
    "# Mode of training: 0 for baseline, 1 for WINN.\n",
    "mode       = 0\n",
    "# GPU index. Default value is 0.\n",
    "gpu        = 1\n",
    "# Number of epochs. Default value is 200.\n",
    "epochs     = 200\n",
    "# Batch size. Default value is 100.\n",
    "batch_size = 100\n",
    "# Number of categories. For MNIST, it is 10.\n",
    "cats       = 10\n",
    "# Number of critics. Default value is 1.\n",
    "critics    = 1\n",
    "# Max number of optimizing steps in synthesis.\n",
    "max_opt_steps = 2000\n",
    "# Image shape. For MNIST, it is [28, 28, 1].\n",
    "image_shape   = [28, 28, 1]\n",
    "\n",
    "# Exported hyper-parameters.\n",
    "half_batch_size = batch_size // 2\n",
    "height, width, channels = image_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T21:50:19.650540Z",
     "start_time": "2018-04-08T21:50:19.638932Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the root dir of data and logs.\n",
    "if   mode == 0:\n",
    "    root_dir = './baseline'\n",
    "elif mode == 1:\n",
    "    root_dir = './winn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T21:50:19.765870Z",
     "start_time": "2018-04-08T21:50:19.651801Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def layer_norm(scope, input_layer, is_training, reuse):\n",
    "    output_layer = tf.contrib.layers.layer_norm(\n",
    "        input_layer,\n",
    "        scale = True,\n",
    "        reuse = reuse,\n",
    "        scope = scope\n",
    "    )\n",
    "    return output_layer\n",
    "\n",
    "def conv2d_res(scope, input_layer, output_dim, use_bias=False,\n",
    "               filter_size=3, strides=[1, 1, 1, 1]):\n",
    "    \n",
    "    input_dim = input_layer.get_shape().as_list()[-1]\n",
    "\n",
    "    with tf.variable_scope(scope):\n",
    "        conv_filter = tf.get_variable(\n",
    "            'conv_weight',\n",
    "            shape = [filter_size, filter_size, input_dim, output_dim],\n",
    "            dtype = tf.float32,\n",
    "            initializer = tf.contrib.layers.variance_scaling_initializer(),\n",
    "            regularizer = tf.contrib.layers.l2_regularizer(scale = 0.0002)\n",
    "        )\n",
    "        conv = tf.nn.conv2d(input_layer, conv_filter, strides, 'SAME')\n",
    "\n",
    "        if use_bias:\n",
    "            bias = tf.get_variable(\n",
    "                'conv_bias',\n",
    "                shape = [output_dim],\n",
    "                dtype = tf.float32,\n",
    "                initializer = tf.constant_initializer(0.0)\n",
    "            )\n",
    "\n",
    "            output_layer = tf.nn.bias_add(conv, bias)\n",
    "            output_layer = tf.reshape(output_layer, conv.get_shape())\n",
    "        else:\n",
    "            output_layer = conv\n",
    "\n",
    "        return output_layer\n",
    "\n",
    "def residual(scope, input_layer, is_training, reuse, \n",
    "             increase_dim=False, first=False):\n",
    "    \n",
    "    input_dim = input_layer.get_shape().as_list()[-1]\n",
    "\n",
    "    if increase_dim:\n",
    "        output_dim = input_dim * 2\n",
    "        strides = [1, 2, 2, 1]\n",
    "    else:\n",
    "        output_dim = input_dim\n",
    "        strides = [1, 1, 1, 1]\n",
    "\n",
    "    with tf.variable_scope(scope):\n",
    "        if first:\n",
    "            h0    = input_layer\n",
    "        else:\n",
    "            h0_ln = layer_norm('h0_ln', input_layer, is_training, reuse)\n",
    "            h0    = swish(h0_ln)\n",
    "\n",
    "        h1_conv = conv2d_res('h1_conv', h0, output_dim, strides=strides)\n",
    "        h1_ln   = layer_norm('h1_ln', h1_conv, is_training, reuse)\n",
    "        h1      = swish(h1_ln)\n",
    "\n",
    "        h2_conv = conv2d_res('h2_conv', h1, output_dim)\n",
    "        if increase_dim:\n",
    "            l = avg_pool('l_pool', input_layer)\n",
    "            l = tf.pad(l, [[0, 0], [0, 0], \n",
    "                           [0, 0], [input_dim // 2, input_dim // 2]])\n",
    "        else:\n",
    "            l = input_layer\n",
    "        h2 = tf.add(h2_conv, l)\n",
    "\n",
    "        return h2\n",
    "    \n",
    "def network(images, is_training, reuse):\n",
    "    with tf.variable_scope('layers', reuse=reuse):\n",
    "        init_dim   = 16\n",
    "        batch_size = images.get_shape().as_list()[0]\n",
    "\n",
    "        r0_conv = conv2d_res('r0_conv', images, init_dim)\n",
    "        r0_ln   = layer_norm('r0_bn', r0_conv, is_training, reuse)\n",
    "        r0      = swish(r0_ln)\n",
    "\n",
    "        r1_res=residual('r1.0', r0, is_training, reuse, first=True)\n",
    "        for k in xrange(1, units):\n",
    "            r1_res = residual('res1.{}'.format(k), r1_res, is_training, reuse)\n",
    "\n",
    "        r2_res=residual('r2.0', r1_res, is_training, reuse, increase_dim=True)\n",
    "        for k in xrange(1, units):\n",
    "            r2_res = residual('res2.{}'.format(k), r2_res, is_training, reuse)\n",
    "\n",
    "        r3_res=residual('r3.0', r2_res, is_training, reuse, increase_dim=True)\n",
    "        for k in xrange(1, units):\n",
    "            r3_res = residual('r3.{}'.format(k), r3_res, is_training, reuse)\n",
    "\n",
    "        r4_bn = layer_norm('r4_ln', r3_res, is_training, reuse)\n",
    "        r4 = swish(r4_bn)\n",
    "\n",
    "        r5 = tf.reduce_mean(r4, axis = [1, 2])\n",
    "\n",
    "        fc = fully_connected('fc', tf.reshape(r5, [batch_size, -1]), 10)\n",
    "        wass = linear(tf.reshape(fc, [batch_size, -1]), 1, 'wass')\n",
    "        return tf.nn.softmax(fc), fc, wass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T21:50:19.913864Z",
     "start_time": "2018-04-08T21:50:19.767639Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_train_op():\n",
    "    # Placeholders.\n",
    "    batch_shape = [batch_size, height, width, channels]\n",
    "    d_real_images_place = tf.placeholder(\n",
    "        tf.float32,\n",
    "        shape = batch_shape,\n",
    "        name = 'd_real_images_place'\n",
    "    )\n",
    "    d_fake_images_place = tf.placeholder(\n",
    "        tf.float32,\n",
    "        shape = batch_shape,\n",
    "        name = 'd_fake_images_place'\n",
    "    )\n",
    "    d_real_labels_place = tf.placeholder(\n",
    "        tf.int32,\n",
    "        shape = [batch_size, ],\n",
    "        name = 'd_real_labels_place'\n",
    "    )\n",
    "    d_fake_labels_place = tf.placeholder(\n",
    "        tf.int32,\n",
    "        shape = [batch_size, ],\n",
    "        name = 'd_fake_labels_place'        \n",
    "    )\n",
    "\n",
    "    # Build network.\n",
    "    d_real_probs, d_real_logits, d_real_wass = \\\n",
    "        network(d_real_images_place, is_training = True,  reuse = False)\n",
    "    d_fake_probs, d_fake_logits, d_fake_wass = \\\n",
    "        network(d_fake_images_place, is_training = False, reuse = True)\n",
    "    \n",
    "    # Loss term 1: Softmax cross-entropy loss.\n",
    "    d_softmax_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels = d_real_labels_place,\n",
    "        logits = d_real_logits\n",
    "    )\n",
    "    d_softmax_loss = tf.reduce_mean(d_softmax_losses)\n",
    "\n",
    "    # Loss term 2: Wasserstein loss with gradient penalty.\n",
    "    # 2.1: Neg. Wasserstein distance. \n",
    "    d_real_loss     = tf.reduce_mean(d_real_wass)\n",
    "    d_fake_loss     = tf.reduce_mean(d_fake_wass)\n",
    "    d_neg_wass_dist = d_fake_loss - d_real_loss \n",
    "    # 2.2: Gradient penalty on interpolated images.\n",
    "    eps = tf.random_uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "    eps = eps + tf.zeros(d_real_images_place.shape, dtype=eps.dtype)\n",
    "    d_inter_images = eps * d_real_images_place + (1-eps) * d_fake_images_place\n",
    "    d_inter_prob, d_inter_logits, d_inter_wass = \\\n",
    "        network(d_inter_images, is_training = False, reuse = True)\n",
    "    d_inter_grad      = tf.gradients(d_inter_wass, d_inter_images)[0]\n",
    "    d_inter_grad_norm = tf.sqrt(tf.reduce_sum(tf.square(d_inter_grad), \n",
    "                                              axis=[1, 2, 3]))\n",
    "    d_inter_grad_penalty = tf.reduce_mean(\n",
    "                           tf.square(d_inter_grad_norm - 1.0))\n",
    "    # 2.3: Wasserstein loss with gradient penalty.\n",
    "    scale = 10.0\n",
    "    d_wass_loss = d_neg_wass_dist + scale * d_inter_grad_penalty\n",
    "    \n",
    "    # Build loss for baseline or WINN using loss terms.\n",
    "    if   mode == 0:\n",
    "        d_loss = d_softmax_loss\n",
    "    elif mode == 1:\n",
    "        d_loss = d_softmax_loss + 0.01 * d_wass_loss\n",
    "\n",
    "    # Optimizer.\n",
    "    d_trainable_vars = [x for x in tf.trainable_variables() \\\n",
    "                          if 'layers' in x.name]\n",
    "    d_optimizer = tf.train.AdamOptimizer(0.001, beta1 = 0.0, beta2 = 0.9)\n",
    "    d_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(d_update_ops):\n",
    "        d_train_op = d_optimizer.minimize(d_loss, var_list = d_trainable_vars)\n",
    "    \n",
    "    return [d_real_images_place, d_fake_images_place, \n",
    "            d_real_labels_place, d_fake_labels_place, \n",
    "            d_loss, d_softmax_loss, d_wass_loss, d_real_loss, d_fake_loss, \n",
    "            d_train_op]\n",
    "\n",
    "def build_synthesis_op():\n",
    "    # Placeholders and variables.\n",
    "    batch_shape = [batch_size, height, width, channels]\n",
    "    g_images = tf.Variable(\n",
    "        np.random.uniform(low = -1.0, \n",
    "                          high = 1.0, \n",
    "                          size = batch_shape\n",
    "        ).astype('float32'), \n",
    "        name='g_images'        \n",
    "    )\n",
    "    g_thres_place = tf.placeholder(\n",
    "        tf.float32,\n",
    "        shape = [],\n",
    "        name = 'g_thres_place'\n",
    "    )        \n",
    "    g_images_place = tf.placeholder(\n",
    "        tf.float32,\n",
    "        shape = g_images.get_shape(),\n",
    "        name = 'g_images_place'\n",
    "    )\n",
    "    g_labels_place = tf.placeholder(\n",
    "        tf.int32,\n",
    "        shape = [batch_size, ],\n",
    "        name = 'g_labels_place'\n",
    "    )\n",
    "    g_images_op = g_images.assign(g_images_place)\n",
    "    \n",
    "    # Build network.\n",
    "    g_probs, g_logits, g_wass=network(g_images, is_training=False, reuse=True)\n",
    "    \n",
    "    # Loss term 1: Softmax cross-entropy loss.\n",
    "    g_softmax_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels = g_labels_place,\n",
    "        logits = g_logits\n",
    "    )\n",
    "    g_softmax_loss = tf.reduce_mean(g_softmax_losses)    \n",
    "    \n",
    "    # Loss term 2: (Absolute) Wasserstein loss.\n",
    "    g_fake_loss      = tf.reduce_mean(g_wass)\n",
    "    g_abs_wass_dist  = tf.abs(g_fake_loss - g_thres_place)\n",
    "    \n",
    "    # Build loss using loss terms.\n",
    "    g_loss           = g_abs_wass_dist + 0.5 * g_softmax_loss\n",
    "    \n",
    "    # Optimizer.\n",
    "    g_trainable_vars = [x for x in tf.trainable_variables() \n",
    "                        if 'g_images' in x.name]\n",
    "    g_optimizer    = tf.train.AdamOptimizer(0.02, beta2 = 0.9)\n",
    "    g_synthesis_op = g_optimizer.minimize(g_loss, var_list = g_trainable_vars)\n",
    "    \n",
    "    return [g_images_place, g_labels_place, g_thres_place, \n",
    "            g_images, g_images_op, g_loss, g_softmax_loss, \n",
    "            g_synthesis_op, g_abs_wass_dist]\n",
    "    \n",
    "def build_test_op():\n",
    "    # Placeholders.\n",
    "    batch_shape = [batch_size, height, width, channels]\n",
    "    c_images_place = tf.placeholder(\n",
    "        tf.float32,\n",
    "        shape = batch_shape,\n",
    "        name = 'c_images_place'\n",
    "    )\n",
    "    c_labels_place = tf.placeholder(\n",
    "        tf.int32,\n",
    "        shape = [batch_size, ],\n",
    "        name = 'c_labels_place'\n",
    "    )\n",
    "    c_probs, c_logits, c_wass = \\\n",
    "        network(c_images_place, is_training = False, reuse = True)\n",
    "    \n",
    "    # Build loss.\n",
    "    c_softmax_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels = c_labels_place,\n",
    "        logits = c_logits\n",
    "    )\n",
    "    c_loss = tf.reduce_mean(c_softmax_losses)\n",
    "    \n",
    "    # Prediction.\n",
    "    c_preds = tf.equal(\n",
    "        tf.cast(tf.argmax(c_probs, axis = 1), tf.int32),\n",
    "        c_labels_place\n",
    "    )\n",
    "    \n",
    "    # Accuracy.\n",
    "    c_acc = tf.reduce_mean(tf.cast(c_preds, tf.float32))\n",
    "    \n",
    "    return [c_images_place, c_labels_place, c_softmax_losses, \n",
    "            c_loss, c_preds, c_acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T21:50:19.995343Z",
     "start_time": "2018-04-08T21:50:19.915493Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dcgan_normalize(name, inputs):\n",
    "    return lib.ops.layernorm.Layernorm(name, [1, 2, 3], inputs)\n",
    "\n",
    "def dcgan_generator(n_samples, noise = None, dim = 64, ln = True):\n",
    "    # Set std for weight initialization in tflib.\n",
    "    lib.ops.conv2d.set_weights_stdev(0.1)\n",
    "    lib.ops.deconv2d.set_weights_stdev(0.1)\n",
    "    lib.ops.linear.set_weights_stdev(0.1)\n",
    "    \n",
    "    # Resize method.\n",
    "    method = tf.image.ResizeMethod.NEAREST_NEIGHBOR \n",
    "    \n",
    "    # DCGAN initialization generator.\n",
    "    with tf.variable_scope(\"dcgan_gen\", reuse = False):\n",
    "        \n",
    "        # External noise or internal noise.\n",
    "        if noise is None: \n",
    "            noise = tf.random_normal([n_samples, 128])\n",
    "\n",
    "        # Linear layer.\n",
    "        x = lib.ops.linear.Linear('dcgan_linear', 128, 4*4*8*dim, noise)\n",
    "        x = tf.reshape(x, [-1, 8*dim, 4, 4])\n",
    "        \n",
    "        # Norm + Conv2D + Resize 1.\n",
    "        if ln: \n",
    "            x = dcgan_normalize('dcgan_ln1',  x)\n",
    "        x = lib.ops.conv2d.Conv2D('dcgan_conv1', 8*dim, 4*dim, 5, x, stride=1)\n",
    "        x = tf.transpose(x, [0, 2, 3, 1])\n",
    "        x = tf.image.resize_images(x, [8, 8], method = method)\n",
    "        x = tf.transpose(x, [0, 3, 1, 2])\n",
    "        \n",
    "        # Norm + Conv2D + Resize 2.\n",
    "        if ln:\n",
    "            x = dcgan_normalize('dcgan_ln2', x)\n",
    "        x = lib.ops.conv2d.Conv2D('dcgan_conv2', 4*dim, 2*dim, 5, x, stride=1)\n",
    "        x = tf.transpose(x, [0, 2, 3, 1])\n",
    "        x = tf.image.resize_images(x, [16, 16], method = method)\n",
    "        x = tf.transpose(x, [0, 3, 1, 2])\n",
    "\n",
    "        # Norm + Conv2D + Resize 3.\n",
    "        if ln: \n",
    "            x = dcgan_normalize('dcgan_ln3', x)\n",
    "        x = lib.ops.conv2d.Conv2D('dcgan_conv3', 2*dim,   dim, 5, x, stride=1)\n",
    "        x = tf.transpose(x, [0, 2, 3, 1])\n",
    "        x = tf.image.resize_images(x, [28, 28], method = method)\n",
    "        x = tf.transpose(x, [0, 3, 1, 2])\n",
    "\n",
    "        # Norm + Conv2D + Resize 4.\n",
    "        if ln: \n",
    "            x = dcgan_normalize('dcgan_ln4', x)\n",
    "        x = lib.ops.conv2d.Conv2D('dcgan_conv4',   dim,     1, 5, x, stride=1)\n",
    "        x = tf.transpose(x, [0, 2, 3, 1])\n",
    "        x = tf.image.resize_images(x, [28, 28], method = method)\n",
    "        x = tf.transpose(x, [0, 3, 1, 2])\n",
    "        \n",
    "        # Tanh non-linearity.\n",
    "        x = tf.tanh(x)\n",
    "        x = tf.transpose(x, [0, 2, 3, 1])\n",
    "        \n",
    "    # Reset std for weight initialization in tflib.\n",
    "    lib.ops.conv2d.unset_weights_stdev()\n",
    "    lib.ops.deconv2d.unset_weights_stdev()\n",
    "    lib.ops.linear.unset_weights_stdev()\n",
    "\n",
    "    return x\n",
    "\n",
    "def build_dcgan_init_op():\n",
    "    # Variable, placeholder and assign operator for multiple generated images.\n",
    "    i_features = tf.Variable(\n",
    "        # Use uniform distribution Unif(-1, 1) to initialize.\n",
    "        # This initialization doesn't matter.\n",
    "        # It will be substituted by i_features_op.\n",
    "        np.random.uniform(low = -1.0,\n",
    "                          high = 1.0, \n",
    "                          size = [batch_size, 128]\n",
    "        ).astype('float32'),\n",
    "        name='i_features'\n",
    "    )\n",
    "    i_features_place = tf.placeholder(dtype = i_features.dtype, \n",
    "                                      shape = i_features.get_shape())\n",
    "    i_features_op    = i_features.assign(i_features_place)\n",
    "    i_dcgan_inits    = dcgan_generator(n_samples = batch_size, \n",
    "                                       noise     = i_features, \n",
    "                                       dim = 64, ln = True)\n",
    "\n",
    "    return i_features, i_features_op, i_features_place, i_dcgan_inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T21:50:20.007651Z",
     "start_time": "2018-04-08T21:50:19.996537Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_matching_images(batch_real_labels, neg_images, neg_labels):\n",
    "    batch_fake_images = []\n",
    "    batch_fake_labels = []\n",
    "    for real_label in batch_real_labels:\n",
    "        select = np.random.randint(0, neg_labels[real_label].shape[0])\n",
    "        batch_fake_images.append(neg_images[real_label][select])\n",
    "        batch_fake_labels.append(real_label)\n",
    "    return np.array(batch_fake_images), np.array(batch_fake_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T21:50:20.530333Z",
     "start_time": "2018-04-08T21:50:20.010423Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(sess):\n",
    "    # Load images from dataset and normalize.\n",
    "    unnorm_all_train_images, all_train_labels = \\\n",
    "        load_train_data()\n",
    "    all_train_images = normalize(unnorm_all_train_images)\n",
    "    train_images, train_labels         = \\\n",
    "        all_train_images[:45000], all_train_labels[:45000]\n",
    "    val_images, val_labels   = \\\n",
    "        all_train_images[45000:], all_train_labels[45000:]\n",
    "    unnorm_test_images, test_labels = \\\n",
    "        load_test_data()\n",
    "    test_images = normalize(unnorm_test_images)\n",
    "    \n",
    "    # Set count of positive images in classifier training in one iteration.\n",
    "    # In fact, we will use the same images in each iteration. \n",
    "    # Besides, the count of negative images in classifier training is the \n",
    "    # same as positive images.\n",
    "    d_pos_images_count    = train_images.shape[0]\n",
    "    \n",
    "    # Set count of pseudo negative images to synthesize in one iteration.\n",
    "    g_neg_images_count    = 1000\n",
    "    g_neg_batches_per_cat = g_neg_images_count // (batch_size * cats)\n",
    "    \n",
    "    # Prepare for training set with specific number.\n",
    "    if d_pos_images_count == train_images.shape[0]:\n",
    "        train_images, train_labels = train_images, train_labels\n",
    "    else:\n",
    "        train_images, train_labels = extract_data(train_images, train_labels, \n",
    "                                                  d_pos_images_count)\n",
    "    \n",
    "    # Create directories for pseudo negative images and models.\n",
    "    if not os.path.exists(root_dir):\n",
    "        os.mkdir(root_dir)\n",
    "    neg_dir   = os.path.join(root_dir, 'neg')\n",
    "    model_dir = os.path.join(root_dir, 'model')\n",
    "    if not os.path.exists(neg_dir):\n",
    "        os.mkdir(neg_dir)\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "    \n",
    "    # Set log file and write experiment settings.\n",
    "    log_file_path = os.path.join(root_dir, 'log.txt')\n",
    "    log(log_file_path, 'Mode: {} (0 for baseline, 1 for WINN).'.format(mode))\n",
    "    log(log_file_path, ('Images: {} for training, {} for validation, ' + \n",
    "                        '{} for test.').format(\n",
    "                           len(train_images), len(val_images), \n",
    "                           len(test_images)))\n",
    "    log(log_file_path, 'Epochs: {}.'.format(epochs))\n",
    "    log(log_file_path, 'Batch size: {}.'.format(batch_size))\n",
    "    log(log_file_path, 'Number of critics: {}.'.format(critics))\n",
    "    log(log_file_path, 'Max. optimizing steps: {}.'.format(max_opt_steps)) \n",
    "    \n",
    "    # Build DCGAN initialization operators.\n",
    "    i_features, i_features_op, i_features_place, i_dcgan_inits = \\\n",
    "        build_dcgan_init_op()\n",
    "    \n",
    "    # Build training operators.\n",
    "    [d_real_images_place, d_fake_images_place, \n",
    "     d_real_labels_place, d_fake_labels_place, \n",
    "     d_loss, d_softmax_loss, d_wass_loss, d_real_loss, d_fake_loss, \n",
    "     d_train_op] = \\\n",
    "        build_train_op()\n",
    "\n",
    "    # Build synthesis operators.\n",
    "    [g_images_place, g_labels_place, g_thres_place, \n",
    "     g_images, g_images_op, g_loss, g_softmax_loss, \n",
    "     g_synthesis_op, g_abs_wass_dist] = \\\n",
    "        build_synthesis_op()\n",
    "\n",
    "    # Build validation/test operators.\n",
    "    [c_images_place, c_labels_place, c_softmax_losses, \n",
    "     c_loss, c_preds, c_acc] = \\\n",
    "        build_test_op()\n",
    "\n",
    "    # After creating network, build saver.\n",
    "    saver = tf.train.Saver(max_to_keep = 100)\n",
    "\n",
    "    # Initialize all variables.\n",
    "    all_initializer_op = tf.global_variables_initializer()\n",
    "    sess.run(all_initializer_op)\n",
    "    \n",
    "    # Prepare initial pseudo negative images.\n",
    "    def get_dcgan_init():\n",
    "        i_batch_features = \\\n",
    "            np.random.uniform(low = -1.0, high = 1.0, size = (batch_size, 128))\n",
    "        sess.run(i_features_op, {i_features_place: i_batch_features})         \n",
    "        i_images = sess.run(i_dcgan_inits)\n",
    "        return i_images\n",
    "        \n",
    "    init_dir = os.path.join(neg_dir, 'init')\n",
    "    if not os.path.exists(init_dir):\n",
    "        os.mkdir(init_dir)\n",
    "    \n",
    "    # Create initial pseudo negative images.\n",
    "    init_images = []\n",
    "    init_labels = []\n",
    "    for cat in xrange(cats):\n",
    "        cat_init_images = []\n",
    "        for _ in xrange(g_neg_batches_per_cat):\n",
    "            batch_images = get_dcgan_init()\n",
    "            cat_init_images.append(batch_images)\n",
    "        cat_init_images = np.concatenate(cat_init_images, axis = 0)  \n",
    "        # Add normalized initial images.\n",
    "        init_images.append(cat_init_images)\n",
    "        init_labels.append(np.full(g_neg_batches_per_cat * batch_size, cat))\n",
    "        assert(len(init_images[-1]) == len(init_labels[-1]))\n",
    "    \n",
    "    # Save all initial images.\n",
    "    for cat in xrange(cats):\n",
    "        unnorm_cat_init_images = unnormalize(init_images[cat])\n",
    "        for batch in xrange(unnorm_cat_init_images.shape[0] // batch_size):\n",
    "            images = unnorm_cat_init_images[   batch    * batch_size : \n",
    "                                            (batch + 1) * batch_size]\n",
    "            images = images.reshape((batch_size, 28, 28))\n",
    "            path   = os.path.join(init_dir, \n",
    "                                  'cat_{}_batch_{}.png'.format(cat, batch))\n",
    "            save_batch_images_to_path(image_shape, images, path)\n",
    "    \n",
    "    # Add normalized initial images into pseudo negative images.\n",
    "    neg_images = init_images\n",
    "    neg_labels = init_labels\n",
    "    \n",
    "    # Log all global variables.\n",
    "    global_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='')\n",
    "    log(log_file_path, 'Global variables:')\n",
    "    for i,var in enumerate(global_vars):\n",
    "        log(log_file_path, '{}, {}, {}.'.format(i, var.name, var.get_shape()))\n",
    "        \n",
    "    # Main procedure.\n",
    "    val_acc_list  = []\n",
    "    test_acc_list = []\n",
    "    for epoch in xrange(epochs):\n",
    "        log(log_file_path, '[{}] Epoch {}.'.format(str(datetime.now()), epoch))\n",
    "        shuffle = np.random.permutation(d_pos_images_count)\n",
    "        \n",
    "        # Classifier training.\n",
    "        for critic in xrange(critics):\n",
    "            \n",
    "            d_critic_loss             = 0.0\n",
    "            d_batches_per_critic      = d_pos_images_count // batch_size\n",
    "            d_batch_real_loss_list    = []\n",
    "            \n",
    "            for batch in xrange(d_batches_per_critic):\n",
    "                \n",
    "                # Positive images to train.\n",
    "                d_batch_real_images = \\\n",
    "                              train_images[shuffle[   batch    * batch_size : \n",
    "                                                   (batch + 1) * batch_size]]\n",
    "                d_batch_real_labels = \\\n",
    "                              train_labels[shuffle[   batch * batch_size : \n",
    "                                                   (batch + 1) * batch_size]]\n",
    "                # Pseudo negative images to train.\n",
    "                d_batch_fake_images, d_batch_fake_labels = \\\n",
    "                    extract_matching_images(d_batch_real_labels, \n",
    "                                            neg_images, neg_labels)\n",
    "                \n",
    "                # Training step.\n",
    "                [_, d_batch_loss, d_batch_softmax_loss, \n",
    "                 d_batch_wass_loss, d_batch_real_loss, d_batch_fake_loss] = \\\n",
    "                    sess.run([d_train_op, d_loss, d_softmax_loss, \n",
    "                              d_wass_loss, d_real_loss, d_fake_loss], \n",
    "                             feed_dict = \\\n",
    "                                 {d_real_images_place: d_batch_real_images, \n",
    "                                  d_fake_images_place: d_batch_fake_images,\n",
    "                                  d_real_labels_place: d_batch_real_labels,\n",
    "                                  d_fake_labels_place: d_batch_fake_labels})\n",
    "                \n",
    "                # Record the loss.\n",
    "                d_batch_real_loss_list.append(d_batch_real_loss)\n",
    "                d_critic_loss += (d_batch_loss / d_batches_per_critic)\n",
    "                \n",
    "            # Log some losses.\n",
    "            log(log_file_path, \n",
    "                '[{}] Critic {}: Discriminator loss {}.'.format(\n",
    "                    str(datetime.now()), critic, d_critic_loss))\n",
    "            log(log_file_path,\n",
    "                ('[{}] Last batch of critic {}: Discriminator loss {}, ' + \n",
    "                 'softmax loss {}, Wass. loss {}, ' + \n",
    "                 'real loss {}, fake loss {}.').format(\n",
    "                    str(datetime.now()), critic, d_batch_loss,\n",
    "                    d_batch_softmax_loss, d_batch_wass_loss, \n",
    "                    d_batch_real_loss, d_batch_fake_loss))\n",
    "        \n",
    "        # Save the model every several epoches.\n",
    "        if epoch % 20 == 0:\n",
    "            checkpoint_path = os.path.join(model_dir, \n",
    "                                           'epoch_{}_model.ckpt'.format(epoch))\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            \n",
    "        # Synthesizing new pseudo negatives. Only enabled under WINN mode.\n",
    "        if mode == 1:\n",
    "            # Directory of synthesized images for current epoch.\n",
    "            neg_epoch_dir = os.path.join(neg_dir, 'epoch_{}'.format(epoch))\n",
    "            if not os.path.exists(neg_epoch_dir):\n",
    "                os.mkdir(neg_epoch_dir)\n",
    "            \n",
    "            for cat in xrange(cats):            \n",
    "                for batch in xrange(g_neg_batches_per_cat):\n",
    "                    g_batch_init_images = get_dcgan_init()\n",
    "                    g_batch_labels = np.full(batch_size, cat)\n",
    "                    \n",
    "                    sess.run(g_images_op, feed_dict = {g_images_place: \n",
    "                                                       g_batch_init_images})\n",
    "                    \n",
    "                    # Choose threshold based on previous real losses.\n",
    "                    g_thres = sess.run(tf.random_uniform(\n",
    "                        [],\n",
    "                        minval = min(d_batch_real_loss_list),\n",
    "                        maxval = max(d_batch_real_loss_list)\n",
    "                    ))\n",
    "        \n",
    "                    for step in xrange(max_opt_steps):\n",
    "                        # Take one step of synthesis.\n",
    "                        sess.run(g_synthesis_op, \n",
    "                                 feed_dict={g_labels_place: g_batch_labels,\n",
    "                                            g_thres_place:  g_thres})\n",
    "                        # Clip the synthesized images.\n",
    "                        sess.run(g_images_op, \n",
    "                                 feed_dict={g_images_place: \n",
    "                                            np.clip(sess.run(g_images), \n",
    "                                                    -1.0, 1.0)})\n",
    "                        # Fetch the loss.\n",
    "                        g_batch_loss, g_batch_abs_wass_dist = \\\n",
    "                            sess.run([g_loss, g_abs_wass_dist],\n",
    "                                feed_dict = {g_labels_place: g_batch_labels,\n",
    "                                             g_thres_place:  g_thres})\n",
    "                        \n",
    "                        # Use (absolute) Wasserstein distance as target.\n",
    "                        if g_batch_abs_wass_dist <= 1e-3:\n",
    "                            break\n",
    "                    \n",
    "                    g_batch_images = sess.run(g_images)\n",
    "                    neg_images[cat] = np.concatenate(\n",
    "                        (neg_images[cat], g_batch_images), axis = 0)\n",
    "                    neg_labels[cat] = np.concatenate(\n",
    "                        (neg_labels[cat], g_batch_labels), axis = 0)\n",
    "                    \n",
    "                    # Save synthesized images.\n",
    "                    images = g_batch_images.reshape((batch_size, 28, 28))\n",
    "                    path   = os.path.join(neg_epoch_dir, \n",
    "                             'cat_{}_batch_{}.png'.format(cat, batch))\n",
    "                    save_batch_images_to_path(image_shape, images, path)\n",
    "                    \n",
    "                    log(log_file_path, \n",
    "                        ('[{}] Generator loss {}, abs. Wass. dist. {}, ' + \n",
    "                         'opt. steps {}.').format(\n",
    "                        str(datetime.now()), g_batch_loss, \n",
    "                            g_batch_abs_wass_dist, step))\n",
    "        \n",
    "        # Validation.\n",
    "        val_loss, val_acc = 0.0, 0.0\n",
    "        val_batches = val_images.shape[0] // batch_size\n",
    "        for batch in xrange(val_batches):\n",
    "            val_batch_images = val_images[   batch    * batch_size : \n",
    "                                          (batch + 1) * batch_size]\n",
    "            val_batch_labels = val_labels[   batch    * batch_size :\n",
    "                                          (batch + 1) * batch_size]\n",
    "            \n",
    "            val_batch_loss, val_batch_acc = \\\n",
    "                sess.run([c_loss, c_acc], \n",
    "                    feed_dict={c_images_place: val_batch_images, \n",
    "                               c_labels_place: val_batch_labels})\n",
    "                \n",
    "            # Batch size should be divisible by count of validation images.\n",
    "            val_loss += (val_batch_loss / val_batches)\n",
    "            val_acc  += (val_batch_acc  / val_batches)\n",
    "\n",
    "        # Save model with max. val. acc.\n",
    "        val_acc_list.append(val_acc)\n",
    "        if val_acc >= max(val_acc_list):\n",
    "            checkpoint = os.path.join(model_dir, \n",
    "                                      'epoch_{}_model.ckpt'.format(epoch))\n",
    "            saver.save(sess, checkpoint)\n",
    "            \n",
    "        # Test.\n",
    "        test_loss, test_acc = 0.0, 0.0\n",
    "        test_batches = test_images.shape[0] // batch_size\n",
    "        for batch in xrange(test_batches):\n",
    "            test_batch_images = test_images[   batch    * batch_size : \n",
    "                                            (batch + 1) * batch_size]\n",
    "            test_batch_labels = test_labels[   batch    * batch_size :\n",
    "                                            (batch + 1) * batch_size]\n",
    "            test_batch_loss, test_batch_acc = \\\n",
    "                sess.run([c_loss, c_acc], \n",
    "                    feed_dict={c_images_place: test_batch_images, \n",
    "                               c_labels_place: test_batch_labels})\n",
    "                \n",
    "            # Batch size should be divisible by count of test images.\n",
    "            test_loss += (test_batch_loss / test_batches)\n",
    "            test_acc  += (test_batch_acc  / test_batches)\n",
    "\n",
    "        # Save model with max. test acc.\n",
    "        test_acc_list.append(test_acc)\n",
    "        if test_acc >= max(test_acc_list):\n",
    "            checkpoint = os.path.join(model_dir, \n",
    "                                      'epoch_{}_model.ckpt'.format(epoch))\n",
    "            saver.save(sess, checkpoint)\n",
    "            \n",
    "        log(log_file_path, \n",
    "            '[{}] Val. loss {}, val. error {}, min. val. error {}.'.format(\n",
    "            str(datetime.now()), val_loss, 1 - val_acc, \n",
    "            1 - max(val_acc_list)))\n",
    "        log(log_file_path, \n",
    "            '[{}] Test loss {}, test error {}, min test error {}.'.format(\n",
    "            str(datetime.now()), test_loss, 1 - test_acc, \n",
    "            1 - max(test_acc_list)))\n",
    "        \n",
    "    log(log_file_path, 'Min test error {}.'.format(1 - max(test_acc_list)))\n",
    "    log(log_file_path, 'Min val. error {}.'.format(1 - max(val_acc_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T01:42:06.460458Z",
     "start_time": "2018-04-08T21:50:20.531663Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu)\n",
    "    # Session configuration.\n",
    "    config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    config.gpu_options.allow_growth = True\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        with tf.device('/gpu:0'):\n",
    "            with tf.Session(config=config) as sess:\n",
    "                with tf.variable_scope('WINN', reuse=None):\n",
    "                    main(sess)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {},
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "threshold": 4,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
